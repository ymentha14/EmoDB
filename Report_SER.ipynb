{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do not spend too much time trying to get very tiny metrics improvement. Once you have a model with a correct predictive power, you should better spend time explaining your data cleaning & preparation pipeline as well as explanations & visualizations of the results.\n",
    "\n",
    "The goal is to see your fit with our company culture & engineering needs, spending 50h on an over-complicated approach will not give you bonus points compared to a simple, yet effective, to-the-point solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset you will be working with is called Emo-DB and can be found [here](http://emodb.bilderbar.info/index-1280.html).\n",
    "\n",
    "It is a database containing samples of emotional speech in German. It contains samples labeled with one of 7 different emotions: Anger, Boredom, Disgust, Fear, Happiness, Sadness and Neutral. \n",
    "\n",
    "Please download the full database and refer to the documentation to understand how the samples are labeled (see \"Additional information\")\n",
    "   \n",
    "The goal of this project is to develop a model which is able to **classify samples of emotional speech**. Feel free to use any available library you would need, but beware of re-using someone else's code without mentionning it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deliverable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The end-goal is to deliver us a zip file containing:\n",
    "* This report filled with your approach, in the form of an **iPython Notebook**.\n",
    "* A **5-10 slides PDF file**, containing a technical presentation covering the important aspects of your work\n",
    "* A Dockerfile which defines a container for the project. The container should handle everything (download the data, run the code, etc...). When running the container it should expose the jupyter notebook on one port and expose a Flask API on another one. The Flask app contains two endpoints:\n",
    "  - One for training the model\n",
    "  - One for querying the last trained model with an audio file of our choice in the dataset\n",
    "* A README.md which should contain the commands to build and run the docker container, as well as how to perform the queries to the API. \n",
    "* Any necessary .py, .sh or other files needed to run your code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from scipy.io import wavfile\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "from IPython.display import clear_output\n",
    "import numpy as np\n",
    "from flask import render_template\n",
    "import traceback\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from IPython.core.debugger import set_trace\n",
    "from speechpy.feature import mfcc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation & Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DE2EN = {'W':'A', #Wut-Anger\n",
    "         'L':'B', #Langeweile-Bordom\n",
    "         'E':'D', #Ekel-Disgust\n",
    "         'A':'F', #Angst-Fear\n",
    "         'F':'H', #Freude-Happiness\n",
    "         'T':'S',\n",
    "         'N':'N'} #Traueer-Sadness\n",
    "EN2DE = {value:key for key,value in DE2EN.items()}\n",
    "EN2NUM = {item[1]:num for item,num in zip(DE2EN.items(),range(len(DE2EN)))}\n",
    "DE2NUM = {item[0]:num for item,num in zip(DE2EN.items(),range(len(DE2EN)))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"./data\"\n",
    "WAV_DIR = os.path.join(DATA_DIR,\"wav\")\n",
    "temp = wavfile.read(\"./data/wav/03a01Fa.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_filename(filename):\n",
    "    \"\"\"\n",
    "    parses the attributes of a given sample based on its filename\n",
    "    \"\"\"\n",
    "    speaker_id = int(filename[:2])\n",
    "    text_id = filename[2:5]\n",
    "    emotion_de = filename[5]\n",
    "    emotion_en = DE2EN[emotion_de]\n",
    "    return speaker_id,text_id,emotion_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "speaker_data = [[3  , 'male',  31],\n",
    "                [8  , 'female',34 ],\n",
    "                [9  , 'female',21 ],\n",
    "                [10 , 'male',  32 ],\n",
    "                [11 , 'male',  26 ],\n",
    "                [12 , 'male',  30] ,\n",
    "                [13 , 'female',32], \n",
    "                [14 , 'female',35] ,\n",
    "                [15 , 'male',  25] ,\n",
    "                [16 , 'female',31]]\n",
    "speaker_data = pd.DataFrame(speaker_data,columns = ['speaker_id','sex','age'])\n",
    "speaker_data.set_index('speaker_id',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pd_data(wav_dir=WAV_DIR):\n",
    "    for root, dirs, files in os.walk(wav_dir, topdown=False):\n",
    "        paths = [os.path.join(root,file) for file in files]\n",
    "        data = []\n",
    "        for file in files:\n",
    "            audio_data = wavfile.read(os.path.join(root,file))[1]\n",
    "            speaker_id,text_id,emotion_en = parse_filename(file)\n",
    "            row = [speaker_id,text_id,emotion_en,audio_data]\n",
    "            data.append(row)\n",
    "    res = pd.DataFrame(data,columns=[\"speaker_id\",\"text_id\",\"emotion\",\"data\"])\n",
    "    return res.join(speaker_data,on=\"speaker_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(wav_dir=WAV_DIR):\n",
    "    data = []\n",
    "    sfs = []\n",
    "    targets = []\n",
    "    for root, dirs, files in os.walk(wav_dir, topdown=False):\n",
    "        for file in files:\n",
    "            sf,audio_data = wavfile.read(os.path.join(root,file))\n",
    "            data.append(audio_data)\n",
    "            sfs.append(sf)\n",
    "            target = DE2NUM[file[5].capitalize()]\n",
    "            targets.append(target)\n",
    "    datapadd = zeropadd(data,mode='mean')\n",
    "    return np.array(sfs),datapadd,np.array(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zeropadd(data,mode='max'):\n",
    "    if mode == 'max':\n",
    "        new_len = max([x.shape[0] for x in data])\n",
    "    else:\n",
    "        new_len = int(np.round(np.mean([x.shape[0] for x in data])))\n",
    "    def padd(x):\n",
    "        diff = abs(new_len - x.shape[0])\n",
    "        shift = diff %2\n",
    "        diff //=2\n",
    "        if x.shape[0] < new_len:\n",
    "            return np.pad(x,(diff,diff+shift),'constant')\n",
    "        else:\n",
    "            return x[diff:-(diff+shift)]\n",
    "    data_padded = np.zeros((len(data),new_len))\n",
    "    for i,x in enumerate(data):\n",
    "        data_padded[i] = padd(x)\n",
    "    return data_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mfcc(data,sfs):\n",
    "    return np.array([mfcc(x,sf,num_cepstral=39) for x,sf in zip(data,sfs)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering & Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummy_model(filename):\n",
    "    data = wavfile.read(os.path.join(WAV_DIR,filename))[1]\n",
    "    return filename[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_classif(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN_classif,self).__init__()\n",
    "        self.convblock1 = nn.Sequential(\n",
    "                                nn.Conv2d(1,8,kernel_size=13),\n",
    "                                nn.BatchNorm2d(8),\n",
    "                                nn.ReLU())\n",
    "        self.convblock2 = nn.Sequential(\n",
    "                                nn.Conv2d(8,8,kernel_size=13),\n",
    "                                nn.BatchNorm2d(8),\n",
    "                                nn.ReLU(),\n",
    "                                nn.MaxPool2d(kernel_size=(2,1)))\n",
    "        self.convblock3 = nn.Sequential(\n",
    "                                nn.Conv2d(8,8,kernel_size=13),\n",
    "                                nn.BatchNorm2d(8),\n",
    "                                nn.ReLU())\n",
    "        self.convblock4 = nn.Sequential(\n",
    "                                nn.Conv2d(8,8,kernel_size=2),\n",
    "                                nn.BatchNorm2d(8),\n",
    "                                nn.ReLU(),\n",
    "                                nn.MaxPool2d(kernel_size=(2,1)))\n",
    "        self.linblock = nn.Sequential(\n",
    "                                nn.Flatten(),\n",
    "                                nn.Linear(896,64),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Dropout(0.2),\n",
    "                                nn.Linear(64,7)\n",
    "        )        \n",
    "    def forward(self,x):\n",
    "        #set_trace()\n",
    "        x = self.convblock1(x)\n",
    "        x = self.convblock2(x)\n",
    "        x = self.convblock3(x)\n",
    "        x = self.convblock4(x)\n",
    "        x = self.linblock(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, inputs, targets):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr = 1e-4)\n",
    "    nb_epochs = 250\n",
    "    batch_size = 20\n",
    "    for e in range(nb_epochs):\n",
    "        clear_output(wait=True)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = outputs.max(1)\n",
    "        accuracy = (predicted == targets).sum().item() / inputs.shape[0] * 100\n",
    "        print(\"Progression:{} % Accuracy: {}% \".format(e/nb_epochs*100,accuracy))\n",
    "        for train_batch,target_batch in zip(inputs.split(batch_size),\n",
    "                                targets.split(batch_size)):\n",
    "            output_batch = model(train_batch)\n",
    "            loss = criterion(output_batch,target_batch)\n",
    "            \n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ymentha/anaconda3/envs/ML/lib/python3.7/site-packages/scipy/io/wavfile.py:273: WavFileWarning: Chunk (non-data) not understood, skipping it.\n",
      "  WavFileWarning)\n"
     ]
    }
   ],
   "source": [
    "sfs,data,targets = load_data()\n",
    "data_f = get_mfcc(data,sfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN_classif()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "data_f = torch.Tensor(data_f).unsqueeze(1)\n",
    "targets = torch.Tensor(targets)\n",
    "train_model(model,data_f,targets.long())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"./models/15_04_1605\")\n",
    "models2 = CNN_classif()\n",
    "models2.load_state_dict(torch.load(\"./models/15_04_1605\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results & Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "loss  = tf.keras.losses.BinaryCrossentropy()\n",
    "\n",
    "loss([0., 0., 1., 1.], [1., 1., 1., 0.])\n",
    "\n",
    "loss2 = nn.NLLLoss()\n",
    "input2 = torch.Tensor([[1.0,0.0],[1.0,0.0], [0.0,1.0],[0.0,1.0]])\n",
    "input2.requires_grad_ = True\n",
    "target2 = torch.Tensor([1,1,1,0]).long()\n",
    "loss2(input2, target2)\n",
    "\n",
    "loss3 = nn.BCELoss(reduction='mean')\n",
    "input3 = torch.Tensor([0., 0., 1., 1.])\n",
    "target3 = torch.Tensor([1., 1., 1., 0.])\n",
    "loss3(input3,target3 )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
