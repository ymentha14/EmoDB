{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do not spend too much time trying to get very tiny metrics improvement. Once you have a model with a correct predictive power, you should better spend time explaining your data cleaning & preparation pipeline as well as explanations & visualizations of the results.\n",
    "\n",
    "The goal is to see your fit with our company culture & engineering needs, spending 50h on an over-complicated approach will not give you bonus points compared to a simple, yet effective, to-the-point solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset you will be working with is called Emo-DB and can be found [here](http://emodb.bilderbar.info/index-1280.html).\n",
    "\n",
    "It is a database containing samples of emotional speech in German. It contains samples labeled with one of 7 different emotions: Anger, Boredom, Disgust, Fear, Happiness, Sadness and Neutral. \n",
    "\n",
    "Please download the full database and refer to the documentation to understand how the samples are labeled (see \"Additional information\")\n",
    "   \n",
    "The goal of this project is to develop a model which is able to **classify samples of emotional speech**. Feel free to use any available library you would need, but beware of re-using someone else's code without mentionning it!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deliverable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The end-goal is to deliver us a zip file containing:\n",
    "* This report filled with your approach, in the form of an **iPython Notebook**.\n",
    "* A **5-10 slides PDF file**, containing a technical presentation covering the important aspects of your work\n",
    "* A Dockerfile which defines a container for the project. The container should handle everything (download the data, run the code, etc...). When running the container it should expose the jupyter notebook on one port and expose a Flask API on another one. The Flask app contains two endpoints:\n",
    "  - One for training the model\n",
    "  - One for querying the last trained model with an audio file of our choice in the dataset\n",
    "* A README.md which should contain the commands to build and run the docker container, as well as how to perform the queries to the API. \n",
    "* Any necessary .py, .sh or other files needed to run your code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "from scipy.io import wavfile\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "from IPython.display import clear_output\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from flask import render_template\n",
    "import traceback\n",
    "from datetime import datetime\n",
    "\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from IPython.core.debugger import set_trace\n",
    "from speechpy.feature import mfcc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation & Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"./data\"\n",
    "WAV_DIR = os.path.join(DATA_DIR,\"wav\")\n",
    "MFCC_DIR  = os.path.join(DATA_DIR,\"mfcc\")\n",
    "\n",
    "DE2EN = {'W':'A', #Wut-Anger\n",
    "         'L':'B', #Langeweile-Bordom\n",
    "         'E':'D', #Ekel-Disgust\n",
    "         'A':'F', #Angst-Fear\n",
    "         'F':'H', #Freude-Happiness\n",
    "         'T':'S',\n",
    "         'N':'N'} #Traueer-Sadness\n",
    "EN2DE = {value:key for key,value in DE2EN.items()}\n",
    "EN2NUM = {item[1]:num for item,num in zip(DE2EN.items(),range(len(DE2EN)))}\n",
    "NUM2EN = {value:key for key,value in EN2NUM.items()}\n",
    "FULL_EM = {'A':'Anger',\n",
    "          'B': 'Bordom',\n",
    "          'D':'Disgust',\n",
    "          'F':'Fear',\n",
    "          'H':'Happiness',\n",
    "          'S':'Sadness',\n",
    "          'N':'Neutral'}\n",
    "\n",
    "DE2NUM = {item[0]:num for item,num in zip(DE2EN.items(),range(len(DE2EN)))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zeropadd(data,mode='max'):\n",
    "    if mode == 'max':\n",
    "        new_len = max([x.shape[0] for x in data])\n",
    "    else:\n",
    "        new_len = int(np.round(np.mean([x.shape[0] for x in data])))\n",
    "    def padd(x):\n",
    "        diff = abs(new_len - x.shape[0])\n",
    "        shift = diff %2\n",
    "        diff //=2\n",
    "        if x.shape[0] < new_len:\n",
    "            return np.pad(x,(diff,diff+shift),'constant')\n",
    "        else:\n",
    "            return x[diff:-(diff+shift)]\n",
    "    data_padded = np.zeros((len(data),new_len))\n",
    "    for i,x in enumerate(data):\n",
    "        data_padded[i] = padd(x)\n",
    "    return data_padded\n",
    "\n",
    "def load_wav_data(wav_dir=WAV_DIR):\n",
    "    data,sfs,targets,file_names = [],[],[],[]\n",
    "    for root, dirs, files in os.walk(wav_dir, topdown=False):\n",
    "        for file in files:\n",
    "            sf,audio_data = wavfile.read(os.path.join(root,file))\n",
    "            data.append(audio_data)\n",
    "            sfs.append(sf)\n",
    "            target = DE2NUM[file[5].capitalize()]\n",
    "            targets.append(target)\n",
    "            file_names.append(file.split(\".\")[0])\n",
    "    data = zeropadd(data,mode='mean')\n",
    "    file_names = np.array(file_names)\n",
    "    sfs = np.array(sfs)\n",
    "    targets = np.array(targets)\n",
    "    order = np.argsort(file_names)\n",
    "    return file_names[order],sfs[order],data[order],targets[order]\n",
    "\n",
    "def get_mfcc(data,sfs):\n",
    "    ret = np.array([mfcc(x,sf,num_cepstral=39) for x,sf in zip(data,sfs)])\n",
    "    return np.expand_dims(ret,axis=1)\n",
    "\n",
    "def save_mfcc_data(file_names,data,targets):\n",
    "    Path(\"data/mfcc\").mkdir(parents=False, exist_ok=True)\n",
    "    for file,smple,target in zip(file_names,data,targets):\n",
    "        file_name = file +\".pkl\"\n",
    "        with open(os.path.join(\"data/mfcc\",file_name),'wb') as f:\n",
    "            save_data = (smple,target)\n",
    "            pickle.dump(save_data,f)\n",
    "    print(\"Saving done!\")\n",
    "\n",
    "def load_mfcc_data(mfcc_dir):\n",
    "    data = []\n",
    "    targets = []\n",
    "    filenames = []\n",
    "    for root, dirs, files in os.walk(mfcc_dir, topdown=False):\n",
    "        for file in files:\n",
    "            with open(os.path.join(root,file),'rb') as f:\n",
    "                temp = pickle.load(f)\n",
    "            data.append(temp[0])\n",
    "            targets.append(temp[1])\n",
    "            filenames.append(file.split('.')[0])\n",
    "    data = np.array(data)\n",
    "    targets = np.array(targets)\n",
    "    filenames = np.array(filenames)\n",
    "    order = np.argsort(filenames)\n",
    "    return filenames[order],data[order],targets[order]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering & Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummy_model(filename):\n",
    "    data = wavfile.read(os.path.join(WAV_DIR,filename))[1]\n",
    "    return filename[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN_classif(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN_classif,self).__init__()\n",
    "        self.convblock1 = nn.Sequential(\n",
    "                                nn.Conv2d(1,8,kernel_size=13),\n",
    "                                nn.BatchNorm2d(8),\n",
    "                                nn.ReLU())\n",
    "        self.convblock2 = nn.Sequential(\n",
    "                                nn.Conv2d(8,8,kernel_size=13),\n",
    "                                nn.BatchNorm2d(8),\n",
    "                                nn.ReLU(),\n",
    "                                nn.MaxPool2d(kernel_size=(2,1)))\n",
    "        self.convblock3 = nn.Sequential(\n",
    "                                nn.Conv2d(8,8,kernel_size=13),\n",
    "                                nn.BatchNorm2d(8),\n",
    "                                nn.ReLU())\n",
    "        self.convblock4 = nn.Sequential(\n",
    "                                nn.Conv2d(8,8,kernel_size=2),\n",
    "                                nn.BatchNorm2d(8),\n",
    "                                nn.ReLU(),\n",
    "                                nn.MaxPool2d(kernel_size=(2,1)))\n",
    "        self.linblock = nn.Sequential(\n",
    "                                nn.Flatten(),\n",
    "                                nn.Linear(896,64),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Dropout(0.2),\n",
    "                                nn.Linear(64,7)\n",
    "        )        \n",
    "    def forward(self,x):\n",
    "        #set_trace()\n",
    "        x = self.convblock1(x)\n",
    "        x = self.convblock2(x)\n",
    "        x = self.convblock3(x)\n",
    "        x = self.convblock4(x)\n",
    "        x = self.linblock(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import LSTM as KERAS_LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KERAS_LSTM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_classif(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LSTM_classif,self).__init__()\n",
    "        #self.lstm = nn.LSTM(num_layers=128,input_size = (int(275),int(39)),\n",
    "                           #hidden_size=128)\n",
    "        self.lstm = nn.LSTM([int(3),int(2)],3)\n",
    "        self.out_layer = nn.Sequential(nn.Dropout(0.5),\n",
    "                                       nn.Linear(1,32),\n",
    "                                       nn.ReLU(),\n",
    "                                       nn.Linear(32,16),\n",
    "                                       nn.Tanh())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "new(): argument 'size' must be tuple of ints, but found element of type list at pos 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-118-e161cacb8db9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlstm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLSTM_classif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-117-6a3ecae9c3ef>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;31m#self.lstm = nn.LSTM(num_layers=128,input_size = (int(275),int(39)),\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                            \u001b[0;31m#hidden_size=128)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         self.out_layer = nn.Sequential(nn.Dropout(0.5),\n\u001b[1;32m      8\u001b[0m                                        \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ML/lib/python3.7/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    502\u001b[0m     \"\"\"\n\u001b[1;32m    503\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'LSTM'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    505\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcheck_forward_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_sizes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ML/lib/python3.7/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, mode, input_size, hidden_size, num_layers, bias, batch_first, dropout, bidirectional)\u001b[0m\n\u001b[1;32m     67\u001b[0m                 \u001b[0mlayer_input_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_size\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mhidden_size\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_directions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0mw_ih\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgate_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_input_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m                 \u001b[0mw_hh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgate_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0mb_ih\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgate_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: new(): argument 'size' must be tuple of ints, but found element of type list at pos 2"
     ]
    }
   ],
   "source": [
    "lstm = LSTM_classif()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/home/ymentha/anaconda3/envs/ML/lib/python3.7/site-packages/torch/nn/modules/rnn.py\u001b[0m(69)\u001b[0;36m__init__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     67 \u001b[0;31m                \u001b[0mlayer_input_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_size\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mhidden_size\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_directions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     68 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 69 \u001b[0;31m                \u001b[0mw_ih\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgate_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayer_input_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     70 \u001b[0;31m                \u001b[0mw_hh\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgate_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     71 \u001b[0;31m                \u001b[0mb_ih\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgate_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  gate_size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  layer_input_size\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(275, 39)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  q\n"
     ]
    }
   ],
   "source": [
    "%debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, inputs, targets,nb_epochs=5):\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr = 1e-4)\n",
    "    batch_size = 20\n",
    "    for e in range(nb_epochs):\n",
    "        clear_output(wait=True)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = outputs.max(1)\n",
    "        accuracy = (predicted == targets).sum().item() / inputs.shape[0] * 100\n",
    "        print(\"Progression:{} % Accuracy: {:.2f}% \".format(e/nb_epochs*100,accuracy))\n",
    "        for train_batch,target_batch in zip(inputs.split(batch_size),\n",
    "                                targets.split(batch_size)):\n",
    "            output_batch = model(train_batch)\n",
    "            loss = criterion(output_batch,target_batch)\n",
    "            \n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(data_f,targets,nb_epochs=5):\n",
    "    \"\"\"\n",
    "    train and save the modle\n",
    "    \"\"\"\n",
    "    model = CNN_classif()\n",
    "    data_f = torch.Tensor(data_f)\n",
    "    targets = torch.Tensor(targets).long()\n",
    "    train_model(model,data_f,targets.long(),nb_epochs)\n",
    "    name = datetime.now().strftime(\"%m_%d_%H%M\")\n",
    "    torch.save(model.state_dict(), \"./models/{}\".format(name))\n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_most_recent(model,model_dir):\n",
    "    filename = max([file  for root, dirs, files in os.walk('./models/', topdown=False) for file in files])\n",
    "    model2.load_state_dict(torch.load(os.path.join(model_dir,filename)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving done!\n"
     ]
    }
   ],
   "source": [
    "file_names,sfs,data,targets = load_wav_data()\n",
    "data_f = get_mfcc(data,sfs)\n",
    "save_mfcc_data(file_names,data_f,targets)\n",
    "file_names,data_f,targets = load_mfcc_data('./data/mfcc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN_classif()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"./models/04_16_1614\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CNN_classif(\n",
       "  (convblock1): Sequential(\n",
       "    (0): Conv2d(1, 8, kernel_size=(13, 13), stride=(1, 1))\n",
       "    (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (convblock2): Sequential(\n",
       "    (0): Conv2d(8, 8, kernel_size=(13, 13), stride=(1, 1))\n",
       "    (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): MaxPool2d(kernel_size=(2, 1), stride=(2, 1), padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (convblock3): Sequential(\n",
       "    (0): Conv2d(8, 8, kernel_size=(13, 13), stride=(1, 1))\n",
       "    (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "  )\n",
       "  (convblock4): Sequential(\n",
       "    (0): Conv2d(8, 8, kernel_size=(2, 2), stride=(1, 1))\n",
       "    (1): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): MaxPool2d(kernel_size=(2, 1), stride=(2, 1), padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (linblock): Sequential(\n",
       "    (0): Flatten()\n",
       "    (1): Linear(in_features=896, out_features=64, bias=True)\n",
       "    (2): ReLU()\n",
       "    (3): Dropout(p=0.2, inplace=False)\n",
       "    (4): Linear(in_features=64, out_features=7, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0087, -0.0942,  0.0515, -0.1189, -0.1157, -0.0567,  0.0173]],\n",
       "       grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(torch.Tensor(data_f[:1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 275, 39)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_f[:1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_model(data_f,targets,nb_epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results & Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sleeping Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "loss  = tf.keras.losses.BinaryCrossentropy()\n",
    "\n",
    "loss([0., 0., 1., 1.], [1., 1., 1., 0.])\n",
    "\n",
    "loss2 = nn.NLLLoss()\n",
    "input2 = torch.Tensor([[1.0,0.0],[1.0,0.0], [0.0,1.0],[0.0,1.0]])\n",
    "input2.requires_grad_ = True\n",
    "target2 = torch.Tensor([1,1,1,0]).long()\n",
    "loss2(input2, target2)\n",
    "\n",
    "loss3 = nn.BCELoss(reduction='mean')\n",
    "input3 = torch.Tensor([0., 0., 1., 1.])\n",
    "target3 = torch.Tensor([1., 1., 1., 0.])\n",
    "loss3(input3,target3 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_pd_data(wav_dir=WAV_DIR):\n",
    "    for root, dirs, files in os.walk(wav_dir, topdown=False):\n",
    "        paths = [os.path.join(root,file) for file in files]\n",
    "        data = []\n",
    "        for file in files:\n",
    "            audio_data = wavfile.read(os.path.join(root,file))[1]\n",
    "            speaker_id,text_id,emotion_en = parse_filename(file)\n",
    "            row = [speaker_id,text_id,emotion_en,audio_data]\n",
    "            data.append(row)\n",
    "    res = pd.DataFrame(data,columns=[\"speaker_id\",\"text_id\",\"emotion\",\"data\"])\n",
    "    return res.join(speaker_data,on=\"speaker_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_filename(filename):\n",
    "    \"\"\"\n",
    "    parses the attributes of a given sample based on its filename\n",
    "    \"\"\"\n",
    "    speaker_id = int(filename[:2])\n",
    "    text_id = filename[2:5]\n",
    "    emotion_de = filename[5]\n",
    "    #emotion_en = DE2EN[emotion_de]\n",
    "    #return speaker_id,text_id,emotion_en\n",
    "    return text_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "speaker_data = [[3  , 'male',  31],\n",
    "                [8  , 'female',34 ],\n",
    "                [9  , 'female',21 ],\n",
    "                [10 , 'male',  32 ],\n",
    "                [11 , 'male',  26 ],\n",
    "                [12 , 'male',  30] ,\n",
    "                [13 , 'female',32], \n",
    "                [14 , 'female',35] ,\n",
    "                [15 , 'male',  25] ,\n",
    "                [16 , 'female',31]]\n",
    "speaker_data = pd.DataFrame(speaker_data,columns = ['speaker_id','sex','age'])\n",
    "speaker_data.set_index('speaker_id',inplace=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
